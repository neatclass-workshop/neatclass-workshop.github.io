---
# layout: page
title: Program
permalink: /program/
---

# Program

The official program will be published after the decisions of paper acceptance have been finalised (29th April 2022).

## Tentative Timeline

The workshop is a full-day meeting and will be held on 6<sup>th</sup> June 2022 in hybrid mode.

All times shown below are in EDT.

* 9.00 Welcome and introductions with ice-breaking questions 
* 9.30 Keynote 1 
* 10.00 Interactive paper presentation 1 
* 10.20 Interactive paper presentation 2 
* 10.40 Interactive paper presentation 3 
* 11.00 Coffee and networking break 
* 11.30 Interactive paper presentation 4 
* 12.50 Interactive paper presentation 5 
* 13.10 Interactive paper presentation 6 
* 13.30 Catered lunch 
* 14:30 Keynote 2 
* 15:00 Challenge: Come up with hard examples that trick models 
* 15:30 Group activity: Where next? Discuss experience with evaluation of models and of human evaluation
* 16:30 Coffee break 
* 17:00 Plenary: Report on group activity 
* 17:30 Closing remarks

## What does Interactive Paper Presentation Mean?

While all the authors of accepted papers will have the chance to record a 20 min presentation of their work that will be published on the workshop website ahead of time, live presentations will be designed to promote interaction. To give space to fresh ideas, participants will be invited to trial an innovative format for paper presentations: presenters will be given 5 minutes to describe their research questions and hypothesis, and a group discussion will start after that (10 minutes). The goal is to give time to the audience to fully appreciate the complexity of the issue targeted in the paper, and propose ideas that have not been biased from the authors' approach. At the end of the discussion, presenters will be given 5 more minutes to describe their method and results, followed by a new group discussion about the interpretation and implications of such results (10 min).

## Challenge

In the afternoon we will kick off with a small challenge: participants will be divided into groups and asked to come up with hard examples that trick existing text classifiers. To this end, we will provide an interactive model for the detection of abusive language and we will invite participants (in advance) to provide their own models and tasks for collaborative adversarial testing. The challenge will work as a warm-up session for the following group activity.

## Group Activity

This last session is meant to bring researchers together and discuss their experience with the evaluation of both models and human annotations. The goal is to collect ideas for new evaluation approaches and future work in the field and to discuss how we should organise competitions when there are multiple evaluation metrics and benchmarking datasets are dynamic. Depending on the number of participants, we will break up into small groups that reconvene after a coffee break to report resulting ideas.
