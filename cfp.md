---
# layout: page
title: Call for Papers
permalink: /call-for-papers/
---

# Call for Papers

Recently there have been attempts to address the problem of benchmarks and metrics that do not represent performance well. For example, in abusive language detection, there are both static datasets of hard-to-detect examples (RÃ¶ttger et al. 2021) and dynamic approaches for generating such examples (Calabrese et al. 2021). On the platform DynaBench (Kiela et al. 2021), benchmarks are dynamic and constantly updated with hard-to-classify examples, avoiding overfitting a predetermined dataset. However, these approaches only capture a tiny fraction of issues with benchmarking. There is still much work to do.

For the first edition of the workshop on Novel Evaluation Approaches for Text Classification Systems (NEATCLasS) we welcome submissions discussing such new evaluation approaches, introducing new or refining existing ones, promoting the use of novel metrics for abuse detection, sentiment analysis and similar tasks within the community. Furthermore, the workshop will promote discussion on the importance, potential and danger of disagreement in tasks that require subjective judgements. This discussion will also focus on how to evaluate human annotations, and how to find the most suitable set of annotators (if any) for a given instance and task. The workshop will solicit, among others, research papers about:
* Issues with current evaluation metrics and benchmarking datasets 
* New evaluation metrics  
* Adaptations and translations of novel evaluation metrics for other languages  
* New data sets for benchmarking  
* Increasing data quality in benchmarking data sets, e.g., avoidance of selection bias, identification of suitable expert human annotators for tasks involving subjective judgements  
* Systems that facilitate dynamic evaluation and benchmarking  
* Models that perform better at hard-to-classify instances and novel evaluation metrics such as AAA, DynaBench and HateCheck  
* Bias, error analysis and model diagnostics  
* Phenomena not captured by existing evaluation metrics (such as models making the right predictions for the wrong reason)  
* Approaches to mitigating bias and common errors  
* Alternative designs for NLP competitions that evaluate a wide range of model characteristics (such as bias, error analysis, cross-domain performance)  
* Challenges of downstream applications (in industry, computational social science, computational communication science, and others) and reflections on how these challenges can be captured in evaluation metrics.

## Contributions

Instructions TBD.

## Submission Information

* Submission link: TBD
* Papers submission deadline: March 27, 2022 
* Paper acceptance notification: April 10, 2022 
* Final camera-ready paper due: April 17, 2022 
* Workshop Day: June 6, 2022

All deadlines are 11:59pm AOE (anywhere on earth).